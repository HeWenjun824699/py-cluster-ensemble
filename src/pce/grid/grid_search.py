import json
import time
import inspect
import logging
import itertools
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Tuple

# Import core components
from .. import io
from .. import consensus
from .. import generators
from .. import metrics
from .. import analysis


class GridSearcher:
    def __init__(self, input_dir: str, output_dir: str):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _filter_kwargs(self, func, all_params: Dict[str, Any]) -> Tuple[Dict[str, Any], List[str]]:
        """
        Core magic: Extract required parameters from a large dictionary based on the function signature.
        Returns: (valid_params, ignored_keys)
        """
        if func is None:
            return {}, []

        sig = inspect.signature(func)
        valid_params = {}
        ignored_keys = []

        func_params = sig.parameters.keys()

        for k, v in all_params.items():
            if k in func_params:
                valid_params[k] = v
            elif k not in ['consensus_method', 'generator_method']:
                # Do not record the method itself, only record the actual parameters
                ignored_keys.append(k)

        return valid_params, ignored_keys

    def _setup_experiment_logger(self, log_path: Path) -> logging.Logger:
        """Create an independent Logger for each experiment folder"""
        logger = logging.getLogger(str(log_path))
        logger.setLevel(logging.INFO)
        logger.handlers = []  # Clear old handlers

        # File Handler (Write to run.log inside the folder)
        fh = logging.FileHandler(log_path, mode='w', encoding='utf-8')
        fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        logger.addHandler(fh)

        return logger

    def _compute_avg_metrics(self, metrics_res: Any) -> Dict[str, float]:
        """Internal function: Process metrics results."""
        if isinstance(metrics_res, list):
            if not metrics_res:
                return {}
            df = pd.DataFrame(metrics_res)
            return df.mean().to_dict()
        elif isinstance(metrics_res, dict):
            return metrics_res
        return {}

    def _prune_combinations(self, raw_combinations: List[Dict], fixed_params: Dict) -> List[Dict]:
        """
        [New Feature] Intelligent Deduplication & Parameter Cleaning
        Logic:
        1. Deduplication: Skip if parameter changes are invalid for the current generator and consensus.
        2. Cleaning: Remove redundant parameters not needed by the current method in the retained tasks.
        """
        valid_tasks = []
        seen_signatures = set()

        print(f"\n>>> Pruning tasks... (Raw combinations: {len(raw_combinations)})")

        for params in raw_combinations:
            full_config = {**fixed_params, **params}

            # 1. Get current method name
            c_method_name = full_config.get('consensus_method', 'unknown')
            g_method_name = full_config.get('generator_method', 'cdkmeans')
            # Convert c_method_name, g_method_name to lowercase
            c_method_name = c_method_name.lower()
            g_method_name = g_method_name.lower()

            # 2. Get function object
            c_func = getattr(consensus, c_method_name, None)
            g_func = getattr(generators, g_method_name, None)

            # 3. Extract effective parameters (Effective Params)
            c_valid, _ = self._filter_kwargs(c_func, full_config)
            g_valid, _ = self._filter_kwargs(g_func, full_config)

            # 4. Generate unique signature (Signature)
            # Signature consists of: Generator Name + Generator Effective Params + Consensus Name + Consensus Effective Params
            # If the signatures generated by two combinations are the same, it means their results are absolutely identical (mathematically equivalent)
            signature = (
                g_method_name,
                json.dumps(g_valid, sort_keys=True),
                c_method_name,
                json.dumps(c_valid, sort_keys=True)
            )

            # =======================================================
            # 1. Pre-calculate "varying parameters" belonging to each method in the current grid
            #    (Place before if/else to ensure both branches can use it)
            # =======================================================
            g_grid_params = {k: v for k, v in params.items() if k in g_valid}
            c_grid_params = {k: v for k, v in params.items() if k in c_valid}

            # 2. Format the display string (define a small function or write logic directly)
            # If dictionary has values, show "name{...}", otherwise show "name"
            g_str = f"{g_method_name}{g_grid_params}" if g_grid_params else g_method_name
            c_str = f"{c_method_name}{c_grid_params}" if c_grid_params else c_method_name

            if signature not in seen_signatures:
                seen_signatures.add(signature)

                # --- 2. Parameter cleaning (Save to task list) ---
                allowed_keys = set(c_valid.keys()) | set(g_valid.keys()) | {'consensus_method', 'generator_method'}
                cleaned_params = {k: v for k, v in params.items() if k in allowed_keys}
                valid_tasks.append(cleaned_params)

                # --- 3. [Main Log] New Task ---
                # Use g_grid_params to only show parameters changing in the current grid search, concise and clear
                print(f"  + [Add Task {len(valid_tasks)}] Config: {g_str} + {c_str}")

            else:
                # --- 4. [Secondary Log] Duplicate Task ---
                # Calculate ignored redundant parameters
                ignored_keys = set(params.keys()) - set(c_valid.keys()) - set(g_valid.keys())
                ignored_details = {k: params[k] for k in ignored_keys}

                if ignored_details:
                    # Also use g_grid_params to show attribution, clearly pointing out which combination this is a duplicate of
                    print(f"    - [Skipped] Duplicate of {g_str} + {c_str}. Ignored: {ignored_details}")
                else:
                    print(f"    - [Skipped] Exact duplicate input.")

        print(f">>> Pruning finished. Effective tasks: {len(valid_tasks)} (Removed {len(raw_combinations) - len(valid_tasks)} redundant tasks)")
        return valid_tasks

    def run(self, param_grid: Dict[str, List[Any]], fixed_params: Dict[str, Any] = None):
        if fixed_params is None: fixed_params = {}

        # ================= NEW: Force fix consensus_method =================
        # Logic: Regardless of whether the user specified consensus_method in param_grid,
        # Move it to fixed_params to prevent Cartesian product or string disassembly.
        if 'consensus_method' in param_grid:
            val = param_grid.pop('consensus_method')

            # Case A: User passed a list (e.g., ["CSPA"]) -> Take the first element
            if isinstance(val, list) and len(val) > 0:
                fixed_params['consensus_method'] = val[0]
                # print(f"\n[Info] 'consensus_method' ({val[0]}) moved from grid to fixed_params.")
            # Case B: User passed a pure string (e.g., "CSPA") -> Use directly
            elif isinstance(val, str):
                fixed_params['consensus_method'] = val
                # print(f"\n[Info] 'consensus_method' ({val}) moved from grid to fixed_params.")
        # =================================================================

        # 1. Generate raw parameter combinations
        keys = param_grid.keys()
        values = param_grid.values()
        raw_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]

        # [Step 1.5: Intelligent Deduplication]
        # Deduplicate here, keeping only truly meaningful combinations
        tasks = self._prune_combinations(raw_combinations, fixed_params)

        print(f"\n>>> Start Grid Search.")
        print(f">>> Output Directory: {self.output_dir}")

        all_summary = []  # Summary table data

        # 2. Iterate through datasets
        mat_files = list(self.input_dir.glob("*.mat"))

        for file_idx, file_path in enumerate(mat_files):
            dataset_name = file_path.stem
            print(f"\n[{file_idx + 1}/{len(mat_files)}] Dataset: {dataset_name}")

            current_dataset_summary = []  # Summary table data for current dataset

            # 2.1 Dataset root directory
            ds_output_dir = self.output_dir / dataset_name
            ds_output_dir.mkdir(exist_ok=True)

            # 2.2 Load data (Cache)
            try:
                try:
                    BPs_cached, Y_cached = io.load_mat_BPs_Y(file_path)
                    data_source = "precomputed"
                except IOError:
                    X_cached, Y_cached = io.load_mat_X_Y(file_path)
                    data_source = "raw"
                    BPs_cached = None
            except Exception as e:
                print(f"  [Error] Failed to load {dataset_name}: {e}")
                continue

            # =======================================================
            # [Modification 1]: Initialize runtime cache variables
            # =======================================================
            last_gen_sig = None  # "Signature" of the last generator
            current_BPs_cache = None  # BPs data currently in memory
            # =======================================================

            # 3. Iterate through deduplicated task list (Tasks)
            for task_idx, params in enumerate(tasks):
                # 3.1 Prepare configuration
                # Naming suggestion: Instead of just using combo_idx, it is better to reflect the characteristics of this parameter set, or directly use task_idx
                c_method = params.get('consensus_method', fixed_params.get('consensus_method', 'unknown'))
                # Convert to lowercase
                c_method = c_method.lower()

                # Use task_idx + 1 to ensure sequential numbering
                exp_id = f"Exp_{task_idx + 1:03d}_{c_method.upper()}"
                exp_dir = ds_output_dir / exp_id
                exp_dir.mkdir(exist_ok=True)

                # Merge all parameters
                full_config = {**fixed_params, **params}

                # 3.2 Initialize single experiment log
                logger = self._setup_experiment_logger(exp_dir / "run.log")
                logger.info(f"===================== Experiment: {exp_id} =====================")
                logger.info(f"Dataset: {dataset_name}")
                logger.info(f"Params: {full_config}")

                start_time = time.time()
                status = "SUCCESS"
                metrics_avg = {}

                try:
                    BPs = None
                    Y = None

                    # --- A. Prepare/Generate Base Clusters (With Cache Logic) ---
                    if data_source == "precomputed":
                        BPs, Y = BPs_cached, Y_cached
                        logger.info("Using precomputed BPs from .mat file.")
                    else:
                        # 1. Extract generator parameters required for the current task
                        g_method = full_config.get('generator_method', 'cdkmeans')
                        # Convert to lowercase
                        g_method = g_method.lower()
                        g_func = getattr(generators, g_method)
                        g_kwargs, _ = self._filter_kwargs(g_func, full_config)
                        # 2. Generate "Signature" for the current task (Method Name + Params JSON)
                        # sort_keys=True ensures consistent signature even if dictionary order differs but content is the same
                        current_sig = (g_method, json.dumps(g_kwargs, sort_keys=True))
                        # 3. Check cache
                        if current_sig == last_gen_sig and current_BPs_cache is not None:
                            # [Cache Hit]
                            logger.info(">> [Cache Hit] Reusing BPs from previous task (Generator params unchanged).")
                            BPs = current_BPs_cache
                        else:
                            # [Cache Miss, Regenerate]
                            logger.info(f">> [Cache Miss] Generating BPs using {g_method.upper()}...")
                            gen_start = time.time()
                            BPs = g_func(X_cached, Y_cached, **g_kwargs)
                            gen_time = time.time() - gen_start
                            logger.info(f"   Generation completed in {gen_time:.4f}s")

                            # Update cache
                            current_BPs_cache = BPs
                            last_gen_sig = current_sig
                        Y = Y_cached

                    # --- B. Cluster Ensemble (Consensus) ---
                    logger.info(f"Running Consensus: {c_method.upper()}...")
                    con_func = getattr(consensus, c_method)
                    con_kwargs, _ = self._filter_kwargs(con_func, full_config)
                    labels, time_list = con_func(BPs, Y, **con_kwargs)
                    logger.info(f"Labels: {labels}")

                    # --- C. Evaluation and Saving ---
                    metrics_res = metrics.evaluation_batch(labels, Y, time_list)
                    logger.info(f"Metrics (Raw): {metrics_res}")
                    # Calculate average of each metric
                    metrics_avg = self._compute_avg_metrics(metrics_res)
                    logger.info(f"Metrics (Avg): {metrics_avg}")

                    # Plot metric line chart
                    # plot_metrics = ["ACC", "NMI", "Purity", "AR", "RI", "MI", "HI", "F-Score", "Precision", "Recall", "Entropy", "SDCS", "RME", "Bal"]
                    plot_metrics = ["ACC", "NMI", "Purity", "AR", "RI", "MI", "HI", "F-Score", "Precision", "Recall", "Entropy", "RME", "Bal"]
                    analysis.plot_metric_line(
                        results_list=metrics_res,
                        metrics=plot_metrics,
                        xlabel='Experiment Run ID',
                        ylabel='Score',
                        title=f"{dataset_name} - {exp_id}",
                        save_path=f"{exp_dir / 'line_plot.png'}",
                        show=False
                    )
                    logger.info(f"Line plot saved in {exp_dir / 'line_plot.png'}")

                    # Save parameters
                    with open(exp_dir / "params.json", 'w') as f:
                        json.dump(full_config, f, indent=4)
                    logger.info(f"Params saved in {exp_dir / 'params.json'}")
                    # Save label results
                    pd.DataFrame(labels).T.to_csv(exp_dir / "labels.csv", index=False, header=False)
                    logger.info(f"Labels saved in {exp_dir / 'labels.csv'}")
                    # Save metrics
                    with open(exp_dir / "metrics.json", 'w') as f:
                        json.dump(metrics_res, f, indent=4)
                    logger.info(f"Scores saved in {exp_dir / 'metrics.json'}")
                    # Save results
                    io.save_results_csv(data=metrics_res, output_path=f"{exp_dir / 'results.csv'}")
                    logger.info(f"Results saved in {exp_dir / 'results.csv'}")

                    logger.info(f"===================== Experiment {exp_id} Completed. =====================")

                except Exception as e:
                    status = "FAILED"
                    logger.error(f"Experiment failed: {e}", exc_info=True)
                    print(f"  x {exp_id} Failed. See log.")

                # Clean up Logger
                handlers = logger.handlers[:]
                for handler in handlers:
                    handler.close()
                    logger.removeHandler(handler)

                # --- 4. Record to summary table ---
                elapsed = time.time() - start_time
                summary_record = {
                    "Dataset": dataset_name,
                    "Exp_id": exp_id,
                    "Status": status,
                    "Total_Time": round(elapsed, 4),
                    **params,
                    **metrics_avg
                }
                current_dataset_summary.append(summary_record)
                all_summary.append(summary_record)

                if status == "SUCCESS":
                    acc = metrics_avg.get('ACC', 0)
                    nmi = metrics_avg.get('NMI', 0)
                    ari = metrics_avg.get('AR', 0)
                    # Simply print progress
                    print(f"  - {exp_id}: ACC={acc:.4f}  NMI={nmi:.4f}  ARI={ari:.4f}  Time={elapsed:.4f}s")

            # =================================================================
            # [Add Code Here]: After loop ends, save Summary for current dataset
            # =================================================================
            if current_dataset_summary:
                save_path = ds_output_dir / f"{dataset_name}.csv"
                pd.DataFrame(current_dataset_summary).to_csv(save_path, index=False)
                print(f"  >> Dataset summary saved: {save_path}")
            # =================================================================

        # 5. Save total summary table
        if all_summary:
            df = pd.DataFrame(all_summary)
            df.to_csv(self.output_dir / "grid_summary.csv", index=False)
            print(f"\nAll Done. Summary saved to {self.output_dir / 'grid_summary.csv'}")